{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Machine Learning Pipeline\n",
    "## Aim here is to create a list of function, meta functions that can work together \n",
    "## to streamline the process of data cleaning, feature selection and training \n",
    "## our models. This will help into creating more results to focus more time on \n",
    "## optimising models and feature selection than the tiny gritty details of coding\n",
    "\n",
    "\n",
    "\n",
    "######### importing python modules \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pylab as plt \n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def df_importer(name):\n",
    "    # import csv from pandas\n",
    "    dat = pd.read_csv(name)\n",
    "    return dat\n",
    "    \n",
    "def convert_cat_2_num(indat,param):\n",
    "    #convert different categories into a numerical order than can be highly correlated\n",
    "    param_lst = ['Po','Fa','TA','Gd','Ex']\n",
    "    for i in range (len(param_lst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == param_lst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1  \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "def convert_GarageFinish(indat,param='GarageFinish'):\n",
    "    #Convert column GarageFinish to a numerically sensible order\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'Fin', param] = 3\n",
    "    except:\n",
    "        dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'RFn', param] = 2\n",
    "    except:\n",
    "        dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'Unf', param] = 1\n",
    "    except:\n",
    "        dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1        \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "\n",
    "def convert_GarageType(indat,param='GarageType'):\n",
    "    #Convert column GarageType to an order related to median prices\n",
    "    garagetypelst = ['CarPort','Detchd','Basment','2Types','Attchd','BuiltIn']\n",
    "    for i in range (len(garagetypelst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == garagetypelst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1        \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "def convert_Neighborhood(indat,param='Neighborhood'):\n",
    "    #Convert column Neighborhood to a numerically to an order related to median prices\n",
    "    neighborlst = ['MeadowV','IDOTRR','BrDale','OldTown','Edwards','BrkSide','Sawyer','Blueste',\n",
    "                   'SWISU','NAmes','NPkVill','Mitchel','SawyerW','Gilbert','NWAmes','Blmngtn',\n",
    "                    'CollgCr','ClearCr','Crawfor','Veenker','Somerst','Timber','StoneBr','NoRidge','NridgHt']\n",
    "    for i in range (len(neighborlst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == neighborlst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1        \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "\n",
    "    return indat\n",
    "\n",
    "def convert_MSZoning(indat,param='MSZoning'):\n",
    "    #Convert column MSZoning to an order related to median prices\n",
    "    param_lst = ['C (all)','RM','RH','RL','FV']\n",
    "    for i in range (len(param_lst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == param_lst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1  \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "def convert_LotShape(indat,param='LotShape'):\n",
    "    #Convert column LotShape to an order related to median prices\n",
    "    param_lst = ['Reg','IR1','IR3','IR2']\n",
    "    for i in range (len(param_lst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == param_lst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1  \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "def convert_BsmtExposure(indat,param='BsmtExposure'):\n",
    "    #Convert column BsmtExposure to an order related to median prices\n",
    "    param_lst = ['No','Mn','Av','Gd']\n",
    "    for i in range (len(param_lst)):\n",
    "        try:\n",
    "            indat.loc[indat[param] == param_lst[i], param] = i+1\n",
    "        except:\n",
    "            dum = 1\n",
    "    try:\n",
    "        indat.loc[indat[param] == 'NA', param] = 0\n",
    "    except:\n",
    "        dum = 1  \n",
    "    try:\n",
    "        indat[param].fillna(0,inplace=True)\n",
    "    except:\n",
    "        dum = 1\n",
    "    return indat\n",
    "\n",
    "\n",
    "def convert_all_categories(indat):\n",
    "    ## meta converter. if used, the following columns of any dataframe will be converted to numerical column:\n",
    "    # 'ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond'\n",
    "    # 'Neighborhood', 'GarageFinish', 'GarageType', 'Neighborhood', 'MSZoning', 'LotShape', 'BsmtExposure'\n",
    "    list_cat_num = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond']\n",
    "    \n",
    "    for i in range(len(list_cat_num)):\n",
    "        convert_cat_2_num(indat,list_cat_num[i])\n",
    "\n",
    "    indat = convert_Neighborhood(indat)\n",
    "    indat = convert_GarageFinish(indat)\n",
    "    indat = convert_GarageType(indat)\n",
    "    indat = convert_Neighborhood(indat)\n",
    "    indat = convert_MSZoning(indat)\n",
    "    indat = convert_LotShape(indat)\n",
    "    indat = convert_BsmtExposure(indat)\n",
    "    return indat\n",
    "\n",
    "\n",
    "def load_converted_df(name):\n",
    "    # load dataframe and convert categorical features into meaningful number.\n",
    "    df = df_importer(name)\n",
    "    df1 = convert_all_categories(df)\n",
    "    return df1\n",
    "\n",
    "def create_spearman_corr_plot(df_in):\n",
    "    # creates the spearman correlation plot for all numerical parameters\n",
    "    plt.style.use('ggplot')\n",
    "    spearman_corr = df_in.corr(method='spearman')\n",
    "    fig = plt.figure(figsize=(10,17))\n",
    "    plt.title('Spearman correlation with sale prices')\n",
    "    spearman_corr['SalePrice'].plot.barh()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_cv(model,df_in,x_arr,y_arr):\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_in.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_arr, y_arr, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "def print_res(selec_mod,mod,df_in,x_arr,y_arr):\n",
    "    score = rmsle_cv(mod,df_in,x_arr,y_arr)\n",
    "    print(\"\\n \"+selec_mod+\" score: {:.4f}% ({:.4f})\\n\".format(score.mean()*100, score.std()*100))\n",
    "    return\n",
    "\n",
    "def ML_models(selec_mod,df_in,x_arr,y_arr,x_tes):\n",
    "    # selec_mod: select model\n",
    "    # df_in - input dataframe - train DF\n",
    "    # x_arr = X_train\n",
    "    # y_arr = Y_train\n",
    "    # x_tes = X_test\n",
    "\n",
    "    if selec_mod == 'lasso':\n",
    "        model_lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "        print_res(selec_mod,model_lasso,df_in,x_arr,y_arr)\n",
    "        model_lasso.fit(x_arr,y_arr)\n",
    "        prediction = model_lasso.predict(x_arr)\n",
    "        prediction1 = model_lasso.predict(x_tes)\n",
    "        print ((1-model_lasso.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_lasso\n",
    "    \n",
    "    if selec_mod == 'Enet':\n",
    "        model_ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "        print_res(selec_mod,model_ENet,df_in,x_arr,y_arr)\n",
    "        model_ENet.fit(x_arr,y_arr)\n",
    "        prediction = model_ENet.predict(x_arr)\n",
    "        prediction1 = model_ENet.predict(x_tes)\n",
    "        print ((1-model_ENet.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_ENet\n",
    "    \n",
    "    if selec_mod == 'KRR':\n",
    "        model_KRR = make_pipeline(RobustScaler(), KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\n",
    "        print_res(selec_mod,model_KRR,df_in,x_arr,y_arr)\n",
    "        model_KRR.fit(x_arr,y_arr)\n",
    "        prediction = model_KRR.predict(x_arr)\n",
    "        prediction1 = model_KRR.predict(x_tes)\n",
    "        print ((1-model_KRR.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_KRR\n",
    "    \n",
    "    if selec_mod == 'GBoost':\n",
    "        model_GBoost = make_pipeline(RobustScaler(), GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5))\n",
    "\n",
    "        print_res(selec_mod,model_GBoost,df_in,x_arr,y_arr)\n",
    "        model_GBoost.fit(x_arr,y_arr)\n",
    "        prediction = model_GBoost.predict(x_arr)\n",
    "        prediction1 = model_GBoost.predict(x_tes)\n",
    "        print ((1-model_GBoost.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_GBoost\n",
    "    \n",
    "    if selec_mod == 'xbg':\n",
    "        model_xgb = make_pipeline(RobustScaler(), xgb.XGBRegressor(colsample_bytree=0.2, gamma=0.0, \n",
    "                             learning_rate=0.01, max_depth=6, \n",
    "                             min_child_weight=1.5, n_estimators=7200,\n",
    "                             reg_alpha=0.9, reg_lambda=0.6,\n",
    "                             subsample=0.2, silent=42,\n",
    "                             random_state =7, nthread = -1))\n",
    "\n",
    "        \n",
    "        print_res(selec_mod,model_xgb,df_in,x_arr,y_arr)\n",
    "        model_xgb.fit(x_arr,y_arr)\n",
    "        prediction = model_xgb.predict(x_arr)\n",
    "        prediction1 = model_xgb.predict(x_tes)\n",
    "        print ((1-model_xgb.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_xgb\n",
    "    \n",
    "    if selec_mod == 'lgb':\n",
    "        model_lgb = make_pipeline(RobustScaler(),lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11))\n",
    "        \n",
    "        print_res(selec_mod,model_lgb,df_in,x_arr,y_arr)\n",
    "        model_lgb.fit(x_arr,y_arr)\n",
    "        prediction = model_lgb.predict(x_arr)\n",
    "        prediction1 = model_lgb.predict(x_tes)\n",
    "        print ((1-model_lgb.score(x_arr,y_arr))*100)\n",
    "        return prediction1,prediction,model_lgb\n",
    "\n",
    "    #if selec_mod == 'randomforrest':\n",
    "    #    model_randomfor = make_pipeline(RobustScaler(), RandomForestRegressor(n_estimators=100,criterion='mse'))\n",
    "\n",
    "    #    print_res(selec_mod,model_randomfor,df_in,x_arr,y_arr)\n",
    "    #    model_randomfor1 = model_randomfor.fit(x_arr,y_arr)\n",
    "    #    prediction = model_randomfor.predict(x_arr)\n",
    "    #    prediction1 = model_randomfor.predict(x_tes)\n",
    "    #    print ((1-model_randomfor.score(x_arr,y_arr))*100)  \n",
    "    #    return prediction1,prediction,model_randomfor1\n",
    "\n",
    "\n",
    "    if selec_mod == 'randomforrest':\n",
    "        model_randomfor =  RandomForestRegressor(n_estimators=1000,criterion='mse')\n",
    "\n",
    "        print_res(selec_mod,model_randomfor,df_in,x_arr,y_arr)\n",
    "        model_randomfor1 = model_randomfor.fit(x_arr,y_arr)\n",
    "        prediction = model_randomfor.predict(x_arr)\n",
    "        prediction1 = model_randomfor.predict(x_tes)\n",
    "        print ((1-model_randomfor.score(x_arr,y_arr))*100)  \n",
    "        return prediction1,prediction,model_randomfor1    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df_in):\n",
    "    #function to use label encoder to encode string columns - returns numerical columns \n",
    "    dfin_str = df_in.select_dtypes(include=['object'])\n",
    "\n",
    "    for col in dfin_str:\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(df_in[col].values)) \n",
    "        df_in[col] = lbl.transform(list(df_in[col].values))\n",
    "        #pd.get_dummies(df_in,prefix=[col], drop_first=True)\n",
    "        df_in = pd.concat([df_in,pd.get_dummies(df_in[col], prefix=col,drop_first=True)],axis=1)\n",
    "        df_in.drop([col],axis=1, inplace=True)\n",
    "    return df_in\n",
    "\n",
    "\n",
    "feature_drop = ['Id','LotFrontage','Alley','MasVnrType','MasVnrArea','BsmtQual','BsmtCond','BsmtExposure',\n",
    "                'BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual',\n",
    "                'GarageCond','PoolQC','Fence','MiscFeature']\n",
    "\n",
    "\n",
    "### next part does not require coding - just go to the feature_importance csv file ... \n",
    "### decide a threshold on which to select columns - in my case I selected everything everything\n",
    "### above 0.1 and created a list from this threshold. This feature list is shown below\n",
    "\n",
    "feature_lst = ['SaleCondition_4','FullBath','BedroomAbvGr','YrSold','ExterQual','HeatingQC','BsmtFullBath',\n",
    "               'EnclosedPorch','MSSubClass','ExterCond','Fireplaces','WoodDeckSF','KitchenQual','TotRmsAbvGrd',\n",
    "               'MoSold','2ndFlrSF','OpenPorchSF','MSZoning','BsmtUnfSF','YearRemodAdd','CentralAir_1',\n",
    "               'OverallCond','YearBuilt','LotArea','GarageCars','BsmtFinSF1','1stFlrSF','GarageArea',\n",
    "               'TotalBsmtSF','Neighborhood','GrLivArea','OverallQual','SalePrice']\n",
    "    \n",
    "    \n",
    "#### using the same old code to load the merged dataset \n",
    "\n",
    "train_test = load_converted_df('train_test_merge.csv')\n",
    "train_test_set = train_test.drop(feature_drop,axis=1)\n",
    "train_test_set = label_encoding(train_test_set)\n",
    "train_test_set = train_test_set.fillna(0)\n",
    "\n",
    "\n",
    "#### but this line is added to only select the columns that met the threshold we selected\n",
    "train_test_set = train_test_set[feature_lst]\n",
    "\n",
    "train = train_test_set.iloc[:1460,:]\n",
    "test = train_test_set.iloc[1460:,:]\n",
    "\n",
    "train.to_csv('new_train1.csv')\n",
    "test.to_csv('new_test1.csv')\n",
    "\n",
    "train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n",
    "train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "Y_train = np.log(train['SalePrice'].values)\n",
    "train.drop(['SalePrice'],axis=1, inplace=True)\n",
    "X_train = train.values\n",
    "\n",
    "X_test = test.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lasso score on training set: ', 0.11713443231885926)\n"
     ]
    }
   ],
   "source": [
    "#simple root mean square\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# they got the best alpha at that value\n",
    "best_alpha = 0.00099\n",
    "\n",
    "#### simple lasso regression based on that alpha value\n",
    "regr = Lasso(alpha=best_alpha, max_iter=50000)\n",
    "regr.fit(X_train,Y_train)\n",
    "y_predl = regr.predict(X_train)\n",
    "y_test = Y_train\n",
    "print(\"Lasso score on training set: \", rmse(y_test, y_predl))\n",
    "\n",
    "y_pred_lasso = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('XGBoost score on training set: ', 0.08845751339590811)\n"
     ]
    }
   ],
   "source": [
    "regr = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=10,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=10,\n",
    "                 silent=1)\n",
    "\n",
    "regr.fit(X_train,Y_train)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_predx = regr.predict(X_train)\n",
    "y_test = Y_train\n",
    "print(\"XGBoost score on training set: \", rmse(y_test, y_predx))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_xgb = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GBoost score: 12.2182% (0.7961)\n",
      "\n",
      "1.3905718849816062\n"
     ]
    }
   ],
   "source": [
    "model_name = 'GBoost'\n",
    "out1G, out2G, out3G = ML_models(model_name,train,X_train,Y_train,X_test)\n",
    "\n",
    "new_array = np.zeros((len(X_test),2),int)\n",
    "testid = pd.read_csv('test.csv')\n",
    "new_array[:,0] = testid['Id'].values\n",
    "new_array[:,1] = np.exp(out1G)\n",
    "np.savetxt(model_name+'4.csv',new_array,delimiter=',',header='Id,SalePrice',comments='',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Enet score: 12.0840% (0.5001)\n",
      "\n",
      "8.56828798413508\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Enet'\n",
    "out1E, out2E, out3E = ML_models(model_name,train,X_train,Y_train,X_test)\n",
    "\n",
    "new_array = np.zeros((len(X_test),2),int)\n",
    "testid = pd.read_csv('test.csv')\n",
    "new_array[:,0] = testid['Id'].values\n",
    "new_array[:,1] = np.exp(out1E)\n",
    "np.savetxt(model_name+'4.csv',new_array,delimiter=',',header='Id,SalePrice',comments='',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " KRR score: 12.5016% (1.1074)\n",
      "\n",
      "5.839758321346178\n"
     ]
    }
   ],
   "source": [
    "model_name = 'KRR'\n",
    "out1K, out2K, out3K = ML_models(model_name,train,X_train,Y_train,X_test)\n",
    "\n",
    "new_array = np.zeros((len(X_test),2),int)\n",
    "testid = pd.read_csv('test.csv')\n",
    "new_array[:,0] = testid['Id'].values\n",
    "new_array[:,1] = np.exp(out1K)\n",
    "np.savetxt(model_name+'4.csv',new_array,delimiter=',',header='Id,SalePrice',comments='',fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Adding all the models we ran together to get an average\n",
    "\n",
    "\n",
    "y_pred = (y_pred_xgb + (out1G) + y_pred_lasso  + out1E + out1K) / 5.\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "testid = pd.read_csv('test.csv')\n",
    "pred_df = pd.DataFrame(y_pred, index=testid[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('gboost_xgb_lasso_Enet_KRR1.csv', header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
